{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from skimage.util import random_noise\n",
    "\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from fastai.callbacks.hooks import *\n",
    "from fastai.utils.mem import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(seed_value, use_cuda):\n",
    "    np.random.seed(seed_value) # cpu vars\n",
    "    torch.manual_seed(seed_value) # cpu  vars\n",
    "    random.seed(seed_value) # Python\n",
    "    if use_cuda: \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
    "        torch.backends.cudnn.deterministic = True  #needed\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_camvid(input, target):\n",
    "    target = target.squeeze(1)\n",
    "    mask = target != void_code\n",
    "    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-asbestos",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(input, target):\n",
    "    input = F.softmax(input, dim=1)\n",
    "    target = torch.squeeze(target,dim=1)\n",
    "\n",
    "    eps = 0.0001\n",
    "    encoded_target = input.detach() * 0\n",
    "    encoded_target = encoded_target.scatter_(1, target.unsqueeze(1), 1)\n",
    "\n",
    "    intersection = input * encoded_target\n",
    "    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n",
    "\n",
    "    denominator = input + encoded_target\n",
    "    denominator = denominator.sum(0).sum(1).sum(1) + eps\n",
    "    loss_per_channel = numerator / denominator\n",
    "\n",
    "    return loss_per_channel.sum() / input.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for a batch of samples\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param output: NxCxHxW logits\n",
    "        :param target: NxHxW LongTensor\n",
    "        :return: torch.tensor\n",
    "        \"\"\"\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return self._dice_loss_multichannel(output,target)\n",
    "\n",
    "    @staticmethod\n",
    "    def _dice_loss_multichannel(output, target):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param output: NxCxHxW Variable\n",
    "        :param target: NxHxW LongTensor\n",
    "        :param weights: C FloatTensor\n",
    "        :param ignore_index: int index to ignore from loss\n",
    "        :param binary: bool for binarized one chaneel(C=1) input\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        target = torch.squeeze(target)\n",
    "        eps = 0.0001\n",
    "        encoded_target = output.detach() * 0\n",
    "\n",
    "        encoded_target = encoded_target.scatter_(1, target.unsqueeze(1), 1)\n",
    "\n",
    "        weights = 1\n",
    "\n",
    "        intersection = output * encoded_target\n",
    "        numerator = 2 * intersection.sum(0).sum(1).sum(1)\n",
    "        denominator = output + encoded_target\n",
    "\n",
    "        denominator = denominator.sum(0).sum(1).sum(1) + eps\n",
    "        loss_per_channel = weights * (1 - (numerator / denominator))\n",
    "\n",
    "        return loss_per_channel.sum() / output.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard pytorch weighted nn.CrossEntropyLoss\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        self.nll_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        :param inputs: torch.tensor (NxC)\n",
    "        :param targets: torch.tensor (N)\n",
    "        :return: scalar\n",
    "        \"\"\"\n",
    "        targets = torch.squeeze(targets)\n",
    "        return self.nll_loss(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A combination of dice  and cross entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.cross_entropy_loss = CrossEntropyLoss2d()\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def forward(self, input, target, weight=True):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        :param input: torch.tensor (NxCxHxW)\n",
    "        :param target: torch.tensor (NxHxW)\n",
    "        :param weight: torch.tensor (NxHxW)\n",
    "        :return: scalar\n",
    "        \"\"\"\n",
    "        weight = self.rtn_weight(torch.squeeze(target))\n",
    "\n",
    "        # input_soft = F.softmax(input, dim=1)\n",
    "        y_2 = torch.mean(self.dice_loss(input, target))\n",
    "        if weight is True:\n",
    "            y_1 = torch.mean(self.cross_entropy_loss.forward(input, target))\n",
    "        else:\n",
    "            y_1 = torch.mean(\n",
    "                torch.mul(self.cross_entropy_loss.forward(input, target), weight))\n",
    "        return y_1 + y_2\n",
    "\n",
    "    def rtn_weight(self, labels):\n",
    "        labels = labels.cpu().numpy()\n",
    "        class_weights = np.zeros_like(labels)\n",
    "\n",
    "        grads = np.gradient(labels) \n",
    "        edge_weights = (grads[0] ** 2 + grads[1] ** 2 ) > 0 \n",
    "        class_weights += 2 * edge_weights\n",
    "        \n",
    "        return torch.from_numpy(class_weights).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_noise(x,use_on_y=True):\n",
    "    x = random_noise(x)\n",
    "    return torch.from_numpy(x).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-score",
   "metadata": {},
   "source": [
    "## path for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random_seed(seed,True)\n",
    "\n",
    "path = Path('./')\n",
    "path_img = Path('./img(random)_10000_3ch')\n",
    "path_lbl = Path('./gt(random)_10000_3ch')\n",
    "fnames = get_image_files(path_img)\n",
    "lbl_names = get_image_files(path_lbl)\n",
    "\n",
    "print(f\"fnames : {fnames[:3]}, label names : {lbl_names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-convert",
   "metadata": {},
   "source": [
    "### Checking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_f = fnames[0]\n",
    "img = open_image(img_f)\n",
    "img.show(figsize=(5,5), cmap='gray')\n",
    "get_y_fn = lambda x: path_lbl/f'{x.stem}_P{x.suffix}'\n",
    "mask = open_mask(get_y_fn(img_f))\n",
    "mask.show(figsize=(5,5), alpha=1)\n",
    "\n",
    "src_size = np.array(mask.shape[1:])\n",
    "print(f\"image size : {src_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-canal",
   "metadata": {},
   "source": [
    "### Label Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.array(['Void', 'Fat', 'Muscle', 'Visceral_fat'], dtype=str); codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-moscow",
   "metadata": {},
   "outputs": [],
   "source": [
    "name2id = {v:k for k,v in enumerate(codes)}\n",
    "void_code = name2id['Void']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-taiwan",
   "metadata": {},
   "source": [
    "### Define Noise for fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = TfmPixel(r_noise)\n",
    "tfms = get_transforms(flip_vert=True, max_rotate=180.0, max_zoom=1.5, max_warp = 0.2 )\n",
    "new_tfms = (tfms[0] + [rn()], tfms[1])\n",
    "new_tfms[0][7].use_on_y = False\n",
    "new_tfms[0][7].p = 0.5\n",
    "size = src_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-conspiracy",
   "metadata": {},
   "source": [
    "### Checking GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "free = gpu_mem_get_free_no_cache()\n",
    "# the max size of bs depends on the available GPU RAM\n",
    "if free > 8200: bs=4\n",
    "else:           bs=2\n",
    "print(f\"using bs={bs}, have {free}MB of GPU RAM free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-devices",
   "metadata": {},
   "source": [
    "### Define DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = (SegmentationItemList.from_folder(path_img)\n",
    "       .split_by_rand_pct(valid_pct=0.1)\n",
    "       .label_from_func(get_y_fn, classes=codes))\n",
    "data = (src.transform(new_tfms, size=size, tfm_y=True)\n",
    "        .databunch(bs=bs, num_workers=0)\n",
    "        .normalize(imagenet_stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-insight",
   "metadata": {},
   "source": [
    "### Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = CombinedLoss\n",
    "metrics = [ dice,acc_camvid ]\n",
    "wd = 1e-2\n",
    "\n",
    "learn = unet_learner(data, models.resnet34, loss_func = loss_func(), metrics=metrics)\n",
    "lr_find(learn)\n",
    "learn.recorder.plot()\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-parameter",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f\"path - \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
